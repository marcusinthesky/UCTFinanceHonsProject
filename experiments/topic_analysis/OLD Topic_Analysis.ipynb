{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: DataFrame to Word Vectors\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.59 s, sys: 1.86 s, total: 5.46 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = os.path.join('.','store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join('..','..','data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\"><mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">South African<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span></mark> President <mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Cyril Ramaphosa<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span></mark> treaded a fine line between his ruling party’s rival factions when he named steady hands to key cabinet posts and purged some of his predecessor <mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Jacob Zuma<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span></mark>’s most ineffectual appointees.  While keeping a handful of <mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Zuma<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span></mark> loyalists in largely minor positions, <mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Ramaphosa<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span></mark> brought back as finance minister <mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Nhlanhla Nene<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span></mark>, who <mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Zuma<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span></mark> replaced in <mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">late 2015<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span></mark> in a decision that caused chaos in the markets. <mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Pravin Gordhan<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span></mark>, who won over investors during <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">two<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span></mark> stints in the same job before running afoul of <mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">Zuma<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span></mark>, will oversee the <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">six<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span></mark> of the biggest state companies that are mostly financially strapped and mired in graft allegations.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(nlp('South African President Cyril Ramaphosa treaded a fine line between his ruling party’s rival factions when he named steady hands to key cabinet posts and purged some of his predecessor Jacob Zuma’s most ineffectual appointees.  While keeping a handful of Zuma loyalists in largely minor positions, Ramaphosa brought back as finance minister Nhlanhla Nene, who Zuma replaced in late 2015 in a decision that caused chaos in the markets. Pravin Gordhan, who won over investors during two stints in the same job before running afoul of Zuma, will oversee the six of the biggest state companies that are mostly financially strapped and mired in graft allegations.') , style='ent', minify=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(data_directory,'sens_data.jsonlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sens_sample = pd.read_hdf('./data/test/big_newsdata.h5', 'sens', mode='a')\n",
    "sens_data = pd.read_json(data_file,convert_dates=True,lines=True,chunksize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/sens_data.jsonlines'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_file = os.path.join(base_directory,'json_lines_lemmas.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-139-250e9fc9b3f6>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-139-250e9fc9b3f6>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    with open(lemma_file, 'w', encoding='utf_8') as f:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def is_punct_helper(token):\n",
    "    \"\"\"helper function to eliminate tokens that are pure punctuation or whitespace\"\"\"\n",
    "    return token.is_punct or token.is_space or token.like_num or token.is_stop or token.like_url or token.like_email\n",
    "\n",
    "\n",
    "def lemmatized_sentence_corpus(generator):    \n",
    "    \"\"\"generator function to use spaCy to parse reviews, lemmatize the text, and yield sentences\"\"\"  \n",
    "    pipe = nlp.pipe(map(lambda x: str(x['text'].values),generator),batch_size=1, n_threads=cpu_count(),disable=['ner','tag','dep'], cleanup=True)\n",
    "    sent = filter(lambda x: x.sents, pipe)\n",
    "    lemmas = (' '.join(map(lambda x: x.lemma_ ,filter(lambda x: not (x.is_punct or x.is_space or x.like_num or x.is_stop or x.like_url or x.like_email or x.is_bracket or x.is_quote or x.is_digit), list(next(sent))[50:-120])))+'\\n')\n",
    "        \n",
    "    with open(lemma_file, 'w', encoding='utf_8') as f:\n",
    "            f.write(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = map(lambda x: x.lemma_ ,filter(lambda x: not (x.is_punct or x.is_space or x.like_num or x.is_stop or x.like_url or x.like_email or x.is_bracket or x.is_quote or x.is_digit), list(next(sent))[50:-120]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.7 ms, sys: 80.7 ms, total: 177 ms\n",
      "Wall time: 76.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmas = (' '.join(map(lambda x: x.lemma_ ,filter(lambda x: not x.is_punct or x.is_space or x.like_num or x.is_stop or x.like_url or x.like_email or x.is_bracket or x.is_quote or x.is_digit, list(next(sent))[50:-120])))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 818 µs, sys: 0 ns, total: 818 µs\n",
      "Wall time: 832 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['samquarz',\n",
       " 'this',\n",
       " 'statement',\n",
       " 'relate',\n",
       " 'cautionary',\n",
       " 'announcement',\n",
       " 'issue',\n",
       " 'petmin',\n",
       " 'on-9',\n",
       " 'january',\n",
       " '2012.-shareholder',\n",
       " 'advise',\n",
       " 'petmin',\n",
       " 'receive',\n",
       " 'notification',\n",
       " 'competition',\n",
       " 'commission',\n",
       " 'relate',\n",
       " 'sale',\n",
       " 'samquarz',\n",
       " 'silica',\n",
       " 'in',\n",
       " 'annual',\n",
       " 'financial',\n",
       " 'statement',\n",
       " 'issue',\n",
       " 'september',\n",
       " 'petmin',\n",
       " 'announce',\n",
       " 'sell',\n",
       " 'samquarz',\n",
       " 'thaba',\n",
       " 'chueu',\n",
       " 'mining',\n",
       " 'pty',\n",
       " 'limited',\n",
       " 'thaba’',\n",
       " 'r259-million',\n",
       " 'subject',\n",
       " 'final',\n",
       " 'approval',\n",
       " 'competition',\n",
       " 'commission',\n",
       " 'department',\n",
       " 'mineral',\n",
       " 'resources',\n",
       " 'on',\n",
       " 'january',\n",
       " 'competition',\n",
       " 'commission',\n",
       " 'notify',\n",
       " 'petmin',\n",
       " 'authorize',\n",
       " 'sale',\n",
       " 'the',\n",
       " 'commission`s',\n",
       " 'decision',\n",
       " 'relate',\n",
       " 'strategic',\n",
       " 'importance',\n",
       " 'samquarz',\n",
       " 'supplier',\n",
       " 'producer',\n",
       " 'ferrosilicon',\n",
       " 'silicon',\n",
       " 'metal',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'the',\n",
       " 'board',\n",
       " 'management',\n",
       " 'petmin',\n",
       " 'thaba',\n",
       " 'study',\n",
       " 'commission`s',\n",
       " 'decision',\n",
       " 'consider',\n",
       " 'option',\n",
       " 'consultation',\n",
       " 'advisor',\n",
       " 'legal',\n",
       " 'counsel',\n",
       " 'the',\n",
       " 'decision',\n",
       " 'impact',\n",
       " 'petmin`s',\n",
       " 'ability',\n",
       " 'deliver',\n",
       " 'strategy',\n",
       " 'include',\n",
       " 'funding',\n",
       " 'current',\n",
       " 'future',\n",
       " 'investment',\n",
       " 'south',\n",
       " 'africa',\n",
       " 'abroad',\n",
       " 'petmin',\n",
       " 'advise',\n",
       " 'shareholder',\n",
       " 'course',\n",
       " 'proceed',\n",
       " 'light',\n",
       " 'commission`s',\n",
       " 'decision',\n",
       " 'enquiry',\n",
       " 'petmin',\n",
       " 'bradley',\n",
       " 'doig',\n",
       " '+',\n",
       " 'enquiry',\n",
       " 'jonathon',\n",
       " 'rees',\n",
       " '+',\n",
       " 'advisor',\n",
       " 'aim)-numis',\n",
       " 'securities',\n",
       " 'limited',\n",
       " '+',\n",
       " '1000-stuart',\n",
       " 'skinner',\n",
       " 'sponsor',\n",
       " 'corporate',\n",
       " 'advisor',\n",
       " 'jse)-river',\n",
       " 'group',\n",
       " 'andrew',\n",
       " 'lianos',\n",
       " '+',\n",
       " '365-johannesburg-16',\n",
       " 'january',\n",
       " '2012-date']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "list(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'portfolio in the newfunds collective investment scheme in securities register as such in term of the collective investment schemes control act 45 of 2002 and manage by newfunds rf)-(proprietary limited newfunds”)’-listing of additional newfunds s&p givi resources index etf participatory interests newfunds have from commencement of business today issue an additional 70,000 s&p givi resources index etf participatory interest at a price of r42.71.-after the additional listing there will be 685,944 s&p givi resources index etf participatory interest in issue.-21 december 2017-sponsor absa bank limited act through -PRON- corporate and investment bank division)-date 21/12/2017 08:00:00 produce by the jse sens department the sens service be an information dissemination service administer by the jse limited jse’).-the jse do not whether expressly tacitly or implicitly represent warrant or in any way guarantee the truth accuracy or completeness of the information publish on sen the jse -PRON- officer employee and agent accept no liability for or in respect of any direct indirect incidental or consequential loss or damage of any kind or nature howsoever arise from the use of sen or the use of or reliance on'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(sent)[11].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lemma file already exists. \n",
      "CPU times: user 1.17 ms, sys: 599 µs, total: 1.76 ms\n",
      "Wall time: 691 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.exists(lemma_file):\n",
    "        print(\"A lemma file already exists. \")\n",
    "else:\n",
    "    lemmatized_sentence_corpus(sens_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Modelling\n",
    "Learning conbinations of tokens that together represent meaningful multi-word concepts.  \n",
    "  \n",
    "$\\frac{count(AB)-count_{min}}{count(a)*count(b)} * N > threashold $\n",
    "\n",
    "Where count is is the number of times a token appears in the corpus.  \n",
    "N is the total size of the corpus.  \n",
    "  \n",
    " Gensim is a library for statistical analysis of sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 356 ms, sys: 15.4 ms, total: 371 ms\n",
      "Wall time: 371 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(lemma_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for unigram_sentence in itertools.islice(unigram_sentences, 235, 240):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_file = os.path.join(base_directory,'bigram_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bigram file already exists. \n",
      "CPU times: user 1.36 ms, sys: 0 ns, total: 1.36 ms\n",
      "Wall time: 1.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.exists(bigram_file):\n",
    "    print(\"A bigram file already exists. \")\n",
    "    bigram_model = Phrases.load(bigram_file)\n",
    "else:\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(base_directory,\n",
    "                                         'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bigram file already exists. \n",
      "CPU times: user 122 µs, sys: 0 ns, total: 122 µs\n",
      "Wall time: 129 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(bigram_sentences_filepath):\n",
    "    print(\"A bigram file already exists. \")\n",
    "else:\n",
    "    with open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:   \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram_sentence in itertools.islice(bigram_sentences, 235, 240):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(base_directory,\n",
    "                                      'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A trigram model already exists. \n",
      "CPU times: user 871 µs, sys: 0 ns, total: 871 µs\n",
      "Wall time: 575 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if os.path.exists(trigram_model_filepath):\n",
    "    print(\"A trigram model already exists. \")\n",
    "    # load the finished model from disk\n",
    "    trigram_model = Phrases.load(trigram_model_filepath)\n",
    "    \n",
    "else:\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(base_directory,\n",
    "                                          'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A trigram model already exists. \n",
      "CPU times: user 604 µs, sys: 0 ns, total: 604 µs\n",
      "Wall time: 363 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.exists(trigram_sentences_filepath):\n",
    "        print(\"A trigram model already exists. \")\n",
    "else:\n",
    "    with open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trigram_sentence in itertools.islice(trigram_sentences, 235, 240):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyldavis in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages\n",
      "Requirement already satisfied: future in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: numexpr in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: funcy in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: pytest in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pyldavis)\n",
      "Requirement already satisfied: setuptools in /home/marcussky/.local/lib/python3.6/site-packages (from pytest->pyldavis)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/marcussky/.local/lib/python3.6/site-packages (from pytest->pyldavis)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pytest->pyldavis)\n",
      "Requirement already satisfied: attrs>=17.2.0 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pytest->pyldavis)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pytest->pyldavis)\n",
      "Requirement already satisfied: python-dateutil>=2 in /home/marcussky/.local/lib/python3.6/site-packages (from pandas>=0.17.0->pyldavis)\n",
      "Requirement already satisfied: pytz>=2011k in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from pandas>=0.17.0->pyldavis)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages (from jinja2>=2.7.2->pyldavis)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(base_directory,\n",
    "                                           'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A trigram dictionary already exists. \n",
      "CPU times: user 746 µs, sys: 321 µs, total: 1.07 ms\n",
      "Wall time: 696 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if os.path.exists(trigram_dictionary_filepath):\n",
    "    print(\"A trigram dictionary already exists. \")\n",
    "    trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)\n",
    "    \n",
    "else:\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_sentences)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    #this was 10 and 0.4\n",
    "    trigram_dictionary.filter_extremes(no_below=15, no_above=0.3)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the finished dictionary from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(base_directory,\n",
    "                                    'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield(trigram_dictionary.doc2bow(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A trigram dictionary already exists. \n",
      "CPU times: user 620 µs, sys: 266 µs, total: 886 µs\n",
      "Wall time: 608 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if os.path.exists(trigram_bow_filepath):\n",
    "        print(\"A trigram dictionary already exists. \")\n",
    "        trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "else:\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_sentences_filepath))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# load the finished bag-of-words corpus from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(base_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages/_pytest/fixtures.py:842: DeprecationWarning: The `convert` argument is deprecated in favor of `converter`.  It will be removed after 2019/01.\n",
      "  params = attr.ib(convert=attr.converters.optional(tuple))\n",
      "/home/marcussky/anaconda3/envs/indigo/lib/python3.6/site-packages/_pytest/fixtures.py:844: DeprecationWarning: The `convert` argument is deprecated in favor of `converter`.  It will be removed after 2019/01.\n",
      "  ids = attr.ib(default=None, convert=_ensure_immutable_ids)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'method' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'method' and 'int'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if os.path.exists(lda_model_filepath):\n",
    "        print(\"A lda model already exists. \")\n",
    "else:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=20,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=(cpu_count-1))\n",
    "    \n",
    "    lda.save(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './store/lda_model_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e0f85c1ec75c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the finished LDA model from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/indigo/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \"\"\"\n\u001b[1;32m   1263\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mmap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mmap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;31m# check if `random_state` attribute has been set after main pickle load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indigo/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indigo/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \"\"\"\n\u001b[0;32m-> 1299\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indigo/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indigo/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './store/lda_model_all'"
     ]
    }
   ],
   "source": [
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=10):\n",
    "    \"\"\"accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\"\"\"\n",
    "        \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_topic(topic_number=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to name the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(base_directory, 'topic_names.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(topic_names_filepath):\n",
    "    print(\"A topic names pickle already exists. \")\n",
    "    with open(topic_names_filepath, 'rb') as f:\n",
    "        topic_names = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    topic_names = {0: u'mexican',\n",
    "               1: u'menu',\n",
    "               2: u'thai',\n",
    "               3: u'steak',\n",
    "               4: u'donuts & appetizers',\n",
    "               5: u'specials',\n",
    "               6: u'soup',\n",
    "               7: u'wings, sports bar',\n",
    "               8: u'foreign language',\n",
    "               9: u'las vegas',\n",
    "               10: u'chicken'\n",
    "               }\n",
    "    with open(topic_names_filepath, 'wb') as f:\n",
    "        pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(base_directory, 'ldavis_prepared')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "\n",
    "if os.path.exists(LDAvis_data_filepath):\n",
    "    print(\"A LDA davis file pickle already exists. \")\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus,\n",
    "                                              trigram_dictionary, n_jobs=-1)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook(local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_visual_output = os.path.join(base_directory, 'ldavis_visual.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(LDAvis_data_filepath):\n",
    "    pass\n",
    "else:\n",
    "    print(\"A LDA visual is being written to disks. \")\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_visual_output, 'wb') as f:\n",
    "        pyLDAvis.save_html(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.show(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
