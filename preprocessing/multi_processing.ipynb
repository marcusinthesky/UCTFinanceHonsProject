{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of multi-processing with Joblib. Here, we're exporting\n",
    "part-of-speech-tagged, true-cased, (very roughly) sentence-separated text, with\n",
    "each \"sentence\" on a newline, and spaces between tokens. Data is loaded from\n",
    "the IMDB movie reviews dataset and will be loaded automatically via Thinc's\n",
    "built-in dataset loader.\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import print_function, unicode_literals\n",
    "from toolz import partition_all\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import thinc.extra.datasets\n",
    "import plac\n",
    "import spacy\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    output_dir=(\"Output directory\", \"positional\", None, Path),\n",
    "    model=(\"Model name (needs tagger)\", \"positional\", None, str),\n",
    "    n_jobs=(\"Number of workers\", \"option\", \"n\", int),\n",
    "    batch_size=(\"Batch-size for each process\", \"option\", \"b\", int),\n",
    "    limit=(\"Limit of entries from the dataset\", \"option\", \"l\", int))\n",
    "def main(output_dir, model='en_core_web_sm', n_jobs=4, batch_size=1000,\n",
    "         limit=10000):\n",
    "    nlp = spacy.load(model)  # load spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    # load and pre-process the IMBD dataset\n",
    "    print(\"Loading IMDB data...\")\n",
    "    data, _ = thinc.extra.datasets.imdb()\n",
    "    texts, _ = zip(*data[-limit:])\n",
    "    print(\"Processing texts...\")\n",
    "    partitions = partition_all(batch_size, texts)\n",
    "    executor = Parallel(n_jobs=n_jobs)\n",
    "    do = delayed(transform_texts)\n",
    "    tasks = (do(nlp, i, batch, output_dir)\n",
    "             for i, batch in enumerate(partitions))\n",
    "    executor(tasks)\n",
    "\n",
    "\n",
    "def transform_texts(nlp, batch_id, texts, output_dir):\n",
    "    print(nlp.pipe_names)\n",
    "    out_path = Path(output_dir) / ('%d.txt' % batch_id)\n",
    "    if out_path.exists():  # return None in case same batch is called again\n",
    "        return None\n",
    "    print('Processing batch', batch_id)\n",
    "    with out_path.open('w', encoding='utf8') as f:\n",
    "        for doc in nlp.pipe(texts):\n",
    "            f.write(' '.join(represent_word(w) for w in doc if not w.is_space))\n",
    "            f.write('\\n')\n",
    "    print('Saved {} texts to {}.txt'.format(len(texts), batch_id))\n",
    "\n",
    "\n",
    "def represent_word(word):\n",
    "    text = word.text\n",
    "    # True-case, i.e. try to normalize sentence-initial capitals.\n",
    "    # Only do this if the lower-cased form is more probable.\n",
    "    if text.istitle() and is_sent_begin(word) \\\n",
    "       and word.prob < word.doc.vocab[text.lower()].prob:\n",
    "        text = text.lower()\n",
    "    return text + '|' + word.tag_\n",
    "\n",
    "\n",
    "def is_sent_begin(word):\n",
    "    if word.i == 0:\n",
    "        return True\n",
    "    elif word.i >= 2 and word.nbor(-1).text in ('.', '!', '?', '...'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
