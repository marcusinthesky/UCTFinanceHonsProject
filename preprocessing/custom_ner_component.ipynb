{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
    "based on list of single or multiple-word company names. Companies are\n",
    "labelled as ORG and their spans are merged into one token. Additionally,\n",
    "._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
    "respectively.\n",
    "* Custom pipeline components: https://spacy.io//usage/processing-pipelines#custom-components\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from pandas import read_csv\n",
    "import os\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    text=(\"Text to process\", \"positional\", None, str),\n",
    "    companies=(\"Names of technology companies\", \"positional\", None, str))\n",
    "def main(text=\"Alphabet Inc. is the company behind Google.\", *companies):\n",
    "    # For simplicity, we start off with only the blank English Language class\n",
    "    # and no model or pre-defined pipeline loaded.\n",
    "    nlp = English()\n",
    "    ner_file = os.path.join('.','named_entities.csv')\n",
    "    if not companies:  # set default companies if none are set via args\n",
    "        companies = read_csv(ner_file)['Company'].tolist() # etc.\n",
    "    component = CompanyRecognizer(nlp, companies)  # initialise component\n",
    "    nlp.add_pipe(component, last=True)  # add last to the pipeline\n",
    "    doc = nlp(text)\n",
    "    print('Pipeline', nlp.pipe_names)  # pipeline contains component name\n",
    "    print('Tokens', [t.text for t in doc])  # company names from the list are merged\n",
    "    print('Doc has_tech_org', doc._.has_org)  # Doc contains tech orgs\n",
    "    print('Token 0 is_tech_org', doc[0]._.is_org)  # \"Alphabet Inc.\" is a tech org\n",
    "    print('Token 1 is_tech_org', doc[1]._.is_org)  # \"is\" is not\n",
    "    print('Entities', [(e.text, e.label_) for e in doc.ents])  # all orgs are entities\n",
    "\n",
    "\n",
    "class CompanyRecognizer(object):\n",
    "    \"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
    "    based on list of single or multiple-word company names. Companies are\n",
    "    labelled as ORG and their spans are merged into one token. Additionally,\n",
    "    ._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
    "    respectively.\"\"\"\n",
    "    name = 'companies'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp, companies=tuple(), label='ORG'):\n",
    "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the shared vocab, get the label ID and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "\n",
    "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
    "        # so even if the list of companies is long, it's very efficient\n",
    "        patterns = [nlp(org) for org in companies]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('ORGS', None, *patterns)\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_org', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_tech_org == True.\n",
    "        Doc.set_extension('has_org', getter=self.has_org)\n",
    "        Span.set_extension('has_org', getter=self.has_org)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_org', True)\n",
    "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token. This is done\n",
    "            # after setting the entities – otherwise, it would cause mismatched\n",
    "            # indices!\n",
    "            span.merge()\n",
    "        return doc  # don't forget to return the Doc!\n",
    "\n",
    "    def has_org(self, tokens):\n",
    "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
    "        is a tech org. Since the getter is only called when we access the\n",
    "        attribute, we can refer to the Token's 'is_tech_org' attribute here,\n",
    "        which is already set in the processing step.\"\"\"\n",
    "        return any([t._.get('is_org') for t in tokens])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Pipeline ['companies']\n",
    "    # Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']\n",
    "    # Doc has_org True\n",
    "    # Token 0 is_org True\n",
    "    # Token 1 is_org False\n",
    "# Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
