{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "# See https://pandoc.org/MANUAL.html#variables-for-latex\n",
    "\n",
    "title: An exploration into the use of Natural Language Processing in Stock Market Prediction on the JSE\n",
    "\n",
    "author:\n",
    "- Robert Brink\n",
    "- Marcus Gawronsky\n",
    "- Christopher Kleyweg\n",
    "\n",
    "#abstract : Investors consider a mosaic of information when making investment decisions, this information often comes in the form of news articles, reports and technical papers.  This paper explores the use of various techniques in the realm of natural language processing to analyze the value is provides in traditional and state-of-the-art predictive models.\n",
    "\n",
    "date: 17 May 2018\n",
    "\n",
    "number_sections: true\n",
    "secnumdepth: 1\n",
    "pagenumbering: true\n",
    "\n",
    "papersize: a4\n",
    "#fontfamily: Latin Modern\n",
    "\n",
    "biblio-title: Bibliography   \n",
    "bibliography: ../../library.bib\n",
    "csl: ./harvard-university-of-cape-town.csl\n",
    "\n",
    "exclude_output: true\n",
    "exclude_input: true\n",
    "exclude_input_prompt: true\n",
    "exclude_output_prompt: true\n",
    "exclude_markdown: False\n",
    "exclude_code_cell: true\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "According to the Efficient Market Hypothesis, markets take into account all relevant information in efficiently pricing securities [@Fama1970]. While many studies have explored this supposition under its strong, semi-strong and weak form, the growing volume, velocity and variety of market data has forced financiers to invest more-and-more in technology as a tool for investor decision making. Investors form a mosaic of information from financial reports, news articles and price data. With the growing trend towards automated trading, a requirement to explore new forms of unstructured and semi-structured data in order to remain competitive has emerged. This literature review aims to explore the use of text data in quantitative stock price prediction, surveying the growing number of techniques in Natural Language Processing to extract evolving market sentiments.  \n",
    "\n",
    "#  Literature Review\n",
    "## Natural Language Processing\n",
    "The Oxford Dictionary contains 180 976 words which can be combined to form a multitude of sentences [@OxfordUniversityPress2018].  While all of these sentences are unique, many share similar meanings.  Algorithms need to find a way of representing these sentences in an efficient and useful manner which captures both the similarity and distinctiveness of these sentences for an array of applications from Human-Computer-Interaction to Document Retrieval.  \n",
    "\n",
    "The realm of Natural Language Processing (NLP) has seen increased attention in recent years with the growth of new techniques, datasets and computing capacity [@Sohangir2018, @OLeary2009, @Cortis2017].  Early applications of NLP include the Bag-of-Words (BOW) approach, a simple technique described by @Harris1954 in which a document is represented by frequency table or matrix, counting the number of times a word appears in a document.  While this approach has many drawbacks [@Harris1954], it does produce document vectors which capture the common meaning of documents sharing the same words.  In English, different words and phrases can share or have different meanings.  A key challenge of NLP comes in how to group these words and phrases in a way which is automated and efficient.  \n",
    "\n",
    "While predefined dictionaries do exist for grouping words, one simple and generalized technique has been through the use of stemming.  Stemming is a non-parametric technique in which common suffixes or prefixes are removed from words based through a set of predefined rules.  In an early algorithm developed by @Lovins1968, suffixes like _'ies'_ are removed from words to reduce the dictionary of words contained in a corpus of documents and better capture their common meaning [@Porter1980, @Rani2015].  \n",
    "\n",
    "[//]: <> (While these techniques have improved over time, a simple parametric version of the algorithm called Lemmatization has grown in popularity in certain applications [@Porter1980, @Rani2015].  Lemmatization aims to stem words based on whether they are a noun or verb in order more accurately group them.  While on small datasets and in certain application Lemmatization has proven effective there still exists much debate as to its value [@Balakrishnan2014].  )\n",
    "\n",
    "In English, phrases can bear new meanings distinct from the words that comprise them.  This phenomenon of _'collocation'_ remains an extensive area of research in NLP [@Lin1998].  Many statistical techniques have been developed to extract these phrases, referred to as n-grams.  While these techniques increase the dictionary of a given corpus, they can improve performance across applications by better capturing the meaning  expressed by documents [@Pecina2005].  \n",
    "\n",
    "One disadvantage in the non-parametric Bag-of-Words technique is that it does not weight words based on their distinctiveness in the corpus of documents.  Articles like _'a'_ or _'the'_ occur throughout all English text, but contribute little meaning to a given sentence.  While much work has been done analysing the information in a given piece of text [@Shannon1951], term-frequency re-weighting has been a common staple with the Bag-of-Words approach to document vectorization [@SparckJones1972, @Robertson2004].  Using Term Frequency-Inverse Document Frequency (TFIDF), the likelihood of choosing a word in a document is multiplied through by its log-likelihood across the corpus.  While this technique can increase the sparsity of a given document vector, it can improve the performance of document similarity measures, valuable in topic analysis and document clustering [@Huang2008].  \n",
    "\n",
    "Unsupervised techniques for dimensionality reduction have been popular extensions on the Bag-of-Words approach.  Papers by @Dumais1988 and @Deerwester1990, through a technique called Latent Semantic Indexing (LSI), compare the use of factor analysis, principal component analysis and multidimensional scaling on count vectors in order to extract continuous document vectors which control for problems of matrix sparsity and polysemy in which many words have the same meaning.  \n",
    "\n",
    "A generative approach by @Blei2003, referred to as Latent Dirichlet Allocation (LDA), uses a three-level hierarchical Bayesian model to extract document vectors from a given corpus.  These document vectors describe each document as a mixture of latent predefined topics. Each word in the corpus has some probability of coming from a given topic and each topic has some probability of appearing in the corpus.  The topics are then mixed according to a Dirichlet Distribution to form each document.  This technique can be computationally challenging but has become a staple in many online applications.  \n",
    "\n",
    "While LSI and LDA have demonstrated significant improvements to document vectorization or embedding, works by @Shannon1951 and @Huang1993 seem to indicate that a word’s meaning can be derived from the contexts it and other words find themselves in rather than just their frequency [@Baroni2014].  These contexts, known as skip-grams, refer to a sequence of words surrounding a target word and may be common for a number of different polysemes.  In the sentence, _'The quick brown fox jumps'_ the skip-gram for _'brown'_ would be _'The quick fox jumps'_.  This skip-gram may repeat again elsewhere in the corpus for the adjective _'red'_, in the sentence _'The quick red fox jumps'_, indicating that _'red'_ and _'brown'_ have similar contexts and may have similar meanings.  \n",
    "\n",
    "In a paper by @Bengio2003, a feed-forward neural network is used with one hidden layer to predict a word's skip-gram [@Alexandrescu2006].  Using the output of this hidden layer, @Bengio2003 demonstrate the value of this approach in extracting rich word-vectors which accurately capture the semantic meaning of words in some continuous vector space.  While this technique remains tractable on small datasets and dictionaries, a breakthrough came with @Mikolov2013 and @Chen2013 who used negative sampling on words' skip-grams as a tool to re-parametrizing the model into something more computationally tractable [@Goldberg2014].  This technique has been extended on by @Quoc2014 in a method commonly referred to as Doc2Vec, which aims to find documents representations by using the same negative sample technique discussed in @Mikolov2013.  \n",
    "\n",
    "In recent years many natural language classification and regression problems have done away with document vectorization or embedding techniques to use deep learning as a tool for automatic feature extraction [@Kim2014, @Luong2013, @Wu2013]. Deep Learning architectures like the Long Short-Term Memory Unit (LSTM) or Convolutional Neural Network (CNN) have shown success in capturing either the locality or sequence-based dependencies contained in this unstructured data.   These techniques may benefit from the distributed representation these models allow for [@Bengio2009].  While there exists limited theoretical studies detailing the factors underlying the success of these models recent works by @Zhu2013 and @Guss2016 relate measures of data complexity from the field of topology to both natural language and the complexity of neural network models providing some justification for their implementation in the field.  \n",
    "\n",
    "## Qualitative Information in Finance  \n",
    "With the ever-increasing availability of qualitative financial information in the form of news articles, blog posts, message boards and financially based social networks investors are no longer able to efficiently monitor and process massive amounts of unstructured data [@Tirunillai2012]. @Engelberg2008 observes that whilst some financial information is still quickly incorporated into markets due to its ease of understanding, other information may be more ambiguous or costly to process and as a result is only reflected in the market over time. Due to this delay in market reaction, hedge funds are beginning to become interested in trading on processed textual data with NLP offering a competitive advantage in both time and cost efficiencies in processing information, allowing for significant profits to be made [@Engelberg2008].  \n",
    "\n",
    "One of the most notable studies in sentiment analysis in the financial industry is that of @Tetlock2007. Tetlock uses a General Inquirer approach, which counts the number of times certain words appear in a text based on predefined categories from Harvard’s psychological dictionary and finds negative words to have a much stronger correlation with stock returns than other words, offering the greatest predicting power for both one-day market and firm returns. In addition, other existing research acknowledges negative word classifications to be the most effective in measuring tone and offers the greatest level of predicting power of financial variables \n",
    "[@Antweiler2002, @Das2007, @Chen2014, @Tetlock2008, @Engelberg2008, @Tirunillai2012]. @Tirunillai2012 suggests a reason for this, believing investors to be more loss averse and hence negative information may elicit a stronger response from investors, whilst they choose to overlook positive information believing it to be more  unreliable. Further developments of Tetlock’s approach included in the innovation of the @Loughran2011 Fin-Neg dictionary, a list of 1202 words which typically have a negative connotation in the financial domain, which proved to be more effective than the Harvard psychological dictionary which considers words to be negative in a general sense [@Cortis2017].  \n",
    "\n",
    "Whilst qualitative news articles can offer investors a simplified explanation of hard to quantify aspect’s of firm fundamentals [@Tetlock2008], ambiguity and implicitly expressed sentiment remain challenging for human interpretation, and even more challenging for computer algorithms acting with limited context [@Das2007].  However, naïve Bayes, a commonly used technique in deriving sentiment from news articles, ignores the challenge presented by context and performs rather well in practice [@Antweiler2002, @Pang2002] only being marginally beaten by other probabilistic classifiers such as Maximum Entropy Classification and SVMs [@Pang2002]. Early significant research conducted by @Gidofalvi2001 uses Naïve Bayes to predict whether a news article is positive, negative or neutral in tone and attempts thereafter to predict the movement of the associated stock. A significant finding from this study reports predictive power in the interval starting 20 minutes prior to the article release and 20 minutes after news articles become publicly available. A later study by @Antweiler2002 attempts to classify stock message boards into buy, sell and hold signals. Whilst the study finds that message boards do not help predict long term stock returns, more bullish messages lead to greater trading volumes followed by negative returns the following day @Hu2015. Another study using Naïve Bayes to classify stock message boards by @Leung2015 demonstrates a size effect as higher message board activity impacts smaller stocks whilst large stocks experience no significant impact. One downfall of Naïve Bayes is that it requires labelled training data which is often difficult to obtain, is costly and impractical [@Leung2015].  \n",
    "\n",
    "@Tirunillai2012 suggests that User Generated Content (UGC) could produce new information about current performance in a more timeously manner as compared to news articles or analyst reports which are released less frequently. In addition, UGC reflects the actual opinions of users of the content as opposed to external influencers, and as a result can be used to determine the opinion of investors over an extended period of time @Tirunillai2012. @Chen2014 identifies the usefulness of peer-based advice in financial markets and suggests that investors may begin to rely on peer based advice instead of more traditional financial analysis [@Sohangir2018]. Potential dangers of this include a wealth transfer from retail investors to institutional investors as less sophisticated traders are more vulnerable to language misprocessing [@Loughran2018] and as a result may not be perfectly rational in their trading decisions.  \n",
    "\n",
    "@OHare2009 develops a corpus of financial blogs in their analysis and use a simple Bag-Of-Words approach to determine sentiment. Because blogs tend to contain emotive opinions where authors offer their own market predictions, the challenge of implicitly expressed sentiment typically found in news articles is avoided. However, it was found that blogs tend to discuss multiple companies and as a result using document level sentiment classification would yield insignificant results. The study provides a solution to this problem leading to the early development of fine-grained analysis, in which company specific sub-documents are extracted and used for training and testing.  \n",
    "\n",
    "In recent developments, the rise of Big Data has resulted in deep learning based approaches to sentiment classification in the financial domain [@Sohangir2018]. Whilst previous studies have used either a rules based text classification approach or a computational statistical approach to classify words into either positive, negative or neutral polarities [@Li2010], few studies thus far have taken into account grammatical structures to perform sentence level sentiment [@Malo2013a].  Prior to the popularity of modern deep learning algorithms, @Malo2013a introduced a concept called “financial entities”, a predefined phrase bank of default neutral polarities that can either take a positive or negative meaning depending on the context, injecting specialized domain knowledge into a tree-kernel learning framework in order to derive sentence level sentiment. Advents of deep learning are further able to address sentence level sentiment through semantic indexing and data tagging, ensuring information pertaining to word order, proximity and relationships is not lost [@Sohangir2018]. Modern approaches in NLP in the financial domain typically include either a lexicon, machine learning, deep learning or hybrid approach [@Cortis2017]. In a recent study,  @Cortis2017 observes Convolutional Neural Networks (CNNs) to be more accurate than LSTM and Doc2Vec approaches in predicting stock market opinions posted in StockTwits. In addition, @Ding2015 further illustrates the capabilities of CNNs to better illustrate the longer term influence of news events when compared to standard feedforward neural networks.  \n",
    "\n",
    "## Market Characteristics\n",
    "In the prediction of stock market prices two main theories dominate the literature: the Efficient Market Hypothesis (EMH) and Random Walk Theory [@Fama1970, @Malkiel1973].  These theories not only discuss the conditional probability and movement of prices but also the ability of the market to assimilated different sources of information. Critical literature by @Osborne1962 and @Fama1970 declare weak-form efficiency as the commonly accepted metric for efficient markets. Using weak-form as a measure of baseline efficiency, Seiler & Rom (1997) and Freud & Pagano (2000) confirm the New York Stock Exchange (NYSE) to be efficient. However, with certain stocks exhibiting a non-random walk behaviour, the NYSE should be considered as less than weak-form efficient. While numerous studies by @VanHeerden2013, @Jefferis2005 and @Smith2002 find the JSE to be weak-form efficient over the time periods 1990 to 2013, similar studies by @VanHeerden2013 and @Phiri2014 dispute these findings demonstrating varying levels of market inefficiency.  A later study by @Noakes2016 confirms this uncertainty, finding certain stocks to exhibit a non-random walk behaviour, similar to the NYSE.  These contrasts in markets efficiency and structure may suggest contrary findings in the use of the Natural Language Models.  \n",
    "\n",
    "While the efficiency of South African Securities remains a continued and important area of research, many studies have analysed mean reversion as a key metric in understanding market over-reaction. A study by @Page1992 on 204 “relatively well traded stocks”  on the JSE between 1974 and 1989 found that losing portfolios yielded stronger mean reversion than winning portfolios, consistent with the hypothesis of over-reaction on the JSE [@Muller1999, @Hsieh2011]. This key phenomenon may suggest event-based studies on the JSE demonstrate results which are more significant or larger in their effect, and may suggest that South African market participants assimilate market information in a way which differs to participants in overseas markets.  \n",
    "\n",
    "The term Investment Style refers to the core investment philosophy adopted by a given market participant [@Robertson2000]. Common styles include value investing where investors purchase securities which they believe have higher intrinsic values than market values and benefiting from market corrections after-the-fact, momentum investing where investors aims to capitalise on current market trends continuing and size investing where investors form a trading strategy around the size of the stocks traded, either small or large capitalization stocks. While studies in South Africa and the United States by @VanRensberg2003, @Basu1977 and @Fama1992 demonstrate the dominance of value and size investment styles across these two markets, works by @Kruger2014 find the consistency of these size effects on the JSE to depend on market stability.  These findings may suggest the dominance of certain financial information in these markets and the consistency of similar studies analysing the effects of qualitative information on market prices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referrences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
