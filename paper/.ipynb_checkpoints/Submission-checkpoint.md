---
title: "Diversification Under Market Inefficiency, an NLP Approach"

author:
- Robert Brink
- Marcus Gawronsky
- Christpher Kleyweg
- Ryan Kruger

date: 19 September 2018

abstract: A recent topic of interest in the realm of financial research has been the use of Artificial Intelligence (AI) in financial prediction. This paper explores the use of various techniques in the realm of Natural Language Processing (NLP), using a popular computational technique in deriving meaning from text data, to analyze the relationship between company association and portfolio diversification for companies on the Johannesburg Stock Exchange (JSE). Using a novel take on the Word2Vec word embedding technique, we show word-vector association between companies to track portfolio volatility over time. 

---  

\newpage
# Introduction  
 
The Capital Asset Pricing Model (CAPM) remains popular amongst financial professionals given its simplicity and ease of use. Central to the CAPM is the assumption of efficient markets, whereby investors are able to fully diversify away unsystematic risk leaving systematic risk represented by Beta as the sole risk determinant [@Strugnell2011]. However, the concept of perfect diversification prescribed by the CAPM remains challenging In South Africa or similar markets which exhibit low forms of efficiency. This leads to an underestimation of risk by South African investors who may be misled in their investment decisions through the use of CAPM or its extensions. This paper, therefore, presents an alternative risk metric known as “Association Risk” which offers investors a way in which to determine the level of diversification of a given portfolio. This metric is devoid of CAPM assumptions, using Natural Language Processing (NLP) techniques on qualitative news articles and analyst reports. This eradicates the problem of quantitative pricing data which may be unreliable or corrupted. Portfolios with lower levels of association are shown to be more diversified, exhibiting lower levels of volatility and therefore lower levels of risk.
 
This paper is organized as follows. Section 2 reviews the literature on the Capital Asset Pricing Model (CAPM) and introduces the use of Natural Language Processing (NLP) in the financial domain. Section 3 details the methodology, data, environment and data preprocessing. Section 4 discusses the model specification, benchmarking, experimental design and method. Section 5 contains the results of this study and is split into two main sections namely Portfolio ANCOVA and Portfolio ANCOVA with time blocking. Finally, in Section 6, we summarise and conclude our findings.  
 
# Literature Review
 
The Capital Asset Pricing Model (CAPM) developed by [@Treynor1961, @Treynor1962, @Sharpe1964, @Lintner1965, @Mossin1966 and @Black1972] remains at the core of modern financial theory by providing investors with a framework in determining how the expected return of an investment is affected but its risk [@Strugnell2011].  
 
$$ \text{Expected Return} = r_{f} + \beta(r_{m} - r_{f})$$  
 
Where Expected Return is the expected returns of a share in the market, $r_{f}$ is the risk-free rate, $r_{m}$ are the returns of the market and $\beta$ is a coefficient computed using through Ordinary Least Squares Regression, under the assumption of normally distributed errors [@Strugnell2011].  

Under CAPM, an asset may only earn a higher average return given an increase in exposure to a comprehensive market portfolio, as denoted by $\beta$, which should capture all systematic risk in the market [@LairdSmith2017]. However, given that the market portfolio which should exist as the universe of all investable assets is not identifiable in reality, a market index is used as a proxy. The performance of CAPM is thus generally poor, especially in small economies such as South Africa where the Johannesburg Stock Exchange (JSE) All-Share Index is not nearly an acceptable proxy, especially in comparison to bigger market such as the S&P 500 [@Bhatnagar2006]. This leaves researchers with a joint hypothesis problem, as researchers are unable to attribute error in their model estimates to either misspecification of the CAPM or the establishment of a valid market portfolio.  
  
If we consider the first hypothesis, around the misspecification of the CAPM Model, numerous papers have critiqued the model's reliance on the assumptions of normality and efficiency both of which have been extensively disproved on the JSE. In a paper by @Smith2005, strong evidence for the existence of leptokurtosis in market returns is presented. While this property is less pronounced in aggregate and over long periods of time, this artifact of market returns violates the common methodologies used in estimating the CAPM, resulting in an underestimate of long-tail down-side risk.  The question of market efficiency is contentious in the literature, given the complexity in determining its hypothesis.  While the findings of @Phiri2014 vary dramatically in their conclusions, the overwhelming evidence by @Smith2005, @VanHeerden2013 and @Smith2002 find the JSE to be, as most, weak-form efficient over the time periods 1990 to 2013.  This result presents a strong argument for the existence of a non-random error term in this model, violating its key assumption that systematic risk is the sole determinant of portfolio risk in a given market. This is supported by existing literature identifying non-systematic anomalies such as the size [@Banz1981] and value effect [@Basu1983], both of which are shown to have reliable predictive power in estimating expected returns. These findings are supported in South Africa by similar research which demonstrate as many as nine characteristic-based stylized facts on the JSE, indicating strongly that sources of non-systematic return exist [@Muller2012, @Kruger2014]. While the application of the CAPM in South Africa may be inappropriate, there is a key assumption it shares with many of its opponents, presented in @Muller2012 and @Kruger2014.  This assumption concerns the existence of a valid and cohesive market portfolio which may account for systematic risk.  This assumption has many lines of argumentation deeply explored within the South African literature. One key criticism raised by researchers has been the ability for investors to construct such a diversified portfolio which samples investable assets to form a valid market proxy due to concerns of over cost, market concentration and liquidity [@Bradfield2004, @Kruger2008].  

The second relies on the ability for investors to construct a cohesive market portfolio which captures some common idea of systematic risk shared between all shares on an index. This may be true in cases where a companies’ decision to list on a particular exchange is either random or consistent with a single predictor.  While this idea of common systematic risk does not necessarily rely on common exposure to a particular economy, it is true that divestment by JSE listed companies in the South African economy and the popularity of dual listings have radically altered the idea and relevance of common systematic risk over a long period of time on the exchange.  A major criticism to the idea of common systematic risk has been thoroughly explored in work by @Page1986, @Venter1992, @Bowie1993, @VanRensburg1997 and @VanRensburg2002 on the correlation between primary risk factors and sector indexes suggesting the JSE All Share Index as an inappropriate measure of systematic risk across all shares in the index. The major criticism raised originally from @Venter1992 comes from the dichotomy between primarily mining and non-mining stocks on the JSE which feature varying sources of risk which appear largely uncorrelated over long periods of time. These papers suggest various methods for constructing and evaluating sector indexes defined according to market correlation and “core-business”. However, these papers are found to vary in their findings as to which index or definition is most appropriate for classifying a company into a particular sector index.  Thin-trading, existing anomalies and market microstructure on the JSE obscure these sectors when applying correlation-based techniques and the strong presence of conglomerates which hold investments across a range of business activities make discrete classifications of company sectors challenging.  The idea of a particular company’s “core-business” changes over time and while on aggregate their definitions may be established, it is difficult to argue that these definitions would not vary according to the personal opinion of various researchers and analysts.  

The literature thus far estimates a linear model against discrete indixes weighted based on a function of market capitalization as opposed to a continuous weighting which exists as a function of market capitalization and company similarity or relevance. This better approximates a cohesive idea of systematic risk where shares do not form a cohesive systematic risk or are not random members of an exchange.  Fundamentally these papers require an appropriate, understandable and unbiased measure of company similarity which is not subject to anomalies and market microstructure and which exists as a continuous metric which provides a smooth and continuous mapping between companies and their industry or similarity to one another.  This metric needs to be computable and should respond to new information.  The metric should also rely on a human-like understanding of the world in order to be both robust in a way which models human behavior over time. While these sector definitions rely only on definitions of “core-business” and correlation, it is unclear as to why they should not include factors such as exposure to various risk factors, sentiment or association.  

Natural Language Processing (NLP) is the application of computational techniques to the analysis and synthesis of natural language in the form of either text or voice data, as opposed to synthetic languages used by computers and researchers. Research into NLP has grown substantially in recent years with the demand of industry and the interest amongst companies into automated chat, virtual assistants, document mining and machine comprehension alongside the growth of new techniques, datasets and computing capacity [@Sohangir2018, @OLeary2009, @Cortis2017]. A central focus of NLP in the financial domain thus far has been sentiment analysis, providing hedge fund managers with a tool in efficiently processing information and hence the ability to profit on potential market inefficiencies [@Engelberg2008]. In practice, however, using sentiment analysis to predict stock prices has shown limited success as ambiguity and implicitly expressed sentiment remain challenging for computer algorithms to interpret. While NLP tools may not be exceptionally accurate at predicting sentiment, they are extremely good at making associations between words and their contexts in text data. The problem with association-based metrics has been the difficulty of extracting association from large datasets of news and research reports as many techniques that try to compute association do not scale to large corpora. Word2Vec scales on text data thanks to its feed-forward neural network re-parameterization and use of negative sampling. We propose a technique which derives an association based portfolio risk metric using unstructured data, using both news articles and analyst reports.  

With the increasing application of  “black-box” models in empirical finance [@Ding2015, @Cortis2017] there exists a growing need for model validation and explainability. Validation which relies on alternative data can be considered the truest extension of backtesting in cases where primary data may be unreliable or corrupted. During periods of extreme market inefficiency markets can no longer be considered an accurate reflection of all public or privately held information, as proposed by the Efficient Market Hypothesis (EMH). This presents major concerns for investors who rely on price information to quantify long-term portfolio risk. Assuming true information exists outside of the marketplace in traditional sources such as qualitative news articles and analyst reports, the question then remains as to how one can then quantify that data in a manner which is free from individual bias and able to be computed reliably and at scale to include all relevant qualitative data in a way that allows an analyst to still estimate and measure long-term portfolio risk.  

By computing a smooth mapping of companies into a vector-space, this paper presents an unexplored use-case for NLP using the Word2Vec algorithm. Using word vectors, we compute a measure of portfolio risk based on the association between companies within a portfolio on the JSE and thus its associated level of diversification under extreme market inefficiency.


 
\newpage 
## Methodology

This paper presents the case for computing a representation of a company’s primary risk factors based on processed text data in the form of news articles and analyst reports.  For a company, primary risk factors can only be identified through access to and synthesis of all public and privately held quantitative and qualitative information. While quantitative information such as company Financial Statements, Market Surveys, and Macroeconomic Data are invaluable sources of primary data, they can only be understood through a deep understanding of their meaning and incorporation in a causal, behavioral and human-derived model of the world.  News and analyst reports aim to harbor relevance through the search and synthesis of this qualitative and quantitative data.   Only through this synthesis can one begin to provide insight into company-specific exposure to primary risk factors.  For businesses their proximity informs their primary risk factors.  Companies discussed in the context of mining, failure or consumer satisfaction for example should contain vastly different characteristics to other companies discussed in other contexts as their proximity to information differs dramatically.  These companies should present varying sources of non-systematic risk based on this information and should serve as a tool in managing non-systematic risk through diversification.  

 
Information in articles and analyst reports is represented primarily and on aggregate through words. The meaning of these words is informed largely by the context they find themselves in.  Words discussed in similar contexts contain similar meaning allowing many to infer meaning from words in this manner.  The Word2vec algorithm assumes this important property of words and context to learn a mathematical representation of words based on the context which surround them, known as a skip-gram [@Mikolov2013b].  By computing vector representations of companies based on the words they are most proximal to, we can compute a vector representation of the information which surrounds them and begin to infer the primary risk factors which emerge in the human synthesis of public information.  Using this method we can compute a relative measure of diversification known as Association Risk.

For investors a primary assumption of risk modeling is in the homogeneity of market properties over time.  While prices may be random, many investors assume the variance of and covariance between shares indicate some common exposure to or nature of non-systematic risk.  This reliance on price and assumption of time homogeneity is a key assumption in all of Finance which breaks down under conditions of extreme market inefficiency.  Under extreme market events, prices are no longer efficient and no longer serve as an accurate measure of long or medium-term portfolio risk.  In the case of the 2010 Flash Crash share prices fell an average of 9\% in the space of 36 minutes [@Borkovec2010] violating many of the assumptions in portfolio modeling and investment.  Under these extreme market conditions market prices no longer provide a valuable medium and long-term insight into market properties, even where clear historical evidence exists, as information contained in news article and reports is no longer incorporated into price as implied by the EMH.  

In order to incorporate this information methods must look to primary sources directly in order to identify drivers of non-systematic risk.  The primary question for researchers then comes in identifying the most appropriate time-horizon over which prices stabilize and investors change their market position.  Long-term studies on the JSE on market liquidity indicate that even under the most extreme conditions investors require an average of 32.12 to trade out of portfolio positions [@Villiers1996].  Given these findings, it is accurate to then argue this liquidity horizon as the most appropriate method for the testing of an alternative measure of portfolio diversification.  

Typically in these crisis situations recent information is most important in identifying the sources of non-systematic risk. While uses of transfer-learning and sample weighting may improve many methods, a 30-day window of historical analysts reports and news data serve as a computable and scalable window for use in long-term testing over large samples from the population of daily news.  This method allows us to compute, on aggregate, an unbiased and efficient smooth mapping between companies and their total risk.  In order to identify the viability of our technique as a tool for investors, we identify the relationship between these levels of Association and portfolio diversification.   

Using random initializing, this algorithm compares a word-vector to the sum of the vectors in its skip-gram to compute a given cosine loss.  Using negative sampling this algorithm efficiently learns a vector representation of these words by sampling words inside and outside this moving skip-gram.  By relying on common company descriptions we can create a Term-Frequency Inverse-Document-Frequency (TFIDF) weighted sum of word-vectors learned on our corpus to predict a company or document-vector, robust to both error and the undersampling of news on particular companies. 

If companies in portfolios lie in vastly different industries and as such are exposed different risk factors, the theory suggests these should add to the diversification of a portfolio - reducing risk.  In order to test our NLP derived measure for quantifying firm-characteristics and their application in portfolio diversification, we analyze the relationship between our metric of association based on historic news and analyst reports against future portfolio volatility.  By controlling for firm-specific volatility characteristics, we compare our historic measure of diversification against the $$ \sum_{i,j=1, i \ne j}^{n} \sigma_{i} \sigma_{j} cov_{i,j} $$ term of the variance sum rule, which corrects for the covariance between shares.

For detailed explanations of Word2Vec, skip-grams, negative sampling and TFIDF see Appendix A.


